{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Lab-2: Deep Learning Experiments\n",
    "\n",
    "## Tasks:\n",
    "- **Task 1:** Compare activation functions (Sigmoid, Tanh, ReLU)\n",
    "- **Task 2:** Compare optimizers (SGD, SGD with Momentum, Adam)\n",
    "- **Task 3:** Test Batch Normalization and Dropout scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "### Setup & Dataset (MNIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {},
   "source": [
    "### Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, activation='relu', use_bn=False, dropout_rate=0.0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Select activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: '{activation}'. Valid options are: 'sigmoid', 'tanh', 'relu'\")\n",
    "        \n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128) if use_bn else None\n",
    "        self.dropout1 = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64) if use_bn else None\n",
    "        self.dropout2 = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        if self.dropout1 is not None:\n",
    "            x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn2(x)\n",
    "        x = self.activation(x)\n",
    "        if self.dropout2 is not None:\n",
    "            x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_header",
   "metadata": {},
   "source": [
    "### Training & Testing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = 100. * correct / len(loader.dataset)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def test(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = 100. * correct / len(loader.dataset)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def run_experiment(model, optimizer, epochs=10, exp_name=\"Experiment\"):\n",
    "    \"\"\"Run training and testing for specified epochs\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    print(f\"\\n{exp_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "            test_loss, test_acc = test(model, test_loader, criterion)\n",
    "            \n",
    "            # Check for NaN values\n",
    "            if np.isnan(train_loss) or np.isnan(test_loss):\n",
    "                print(f\"\\nWarning: NaN detected at epoch {epoch+1}. Stopping training.\")\n",
    "                break\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:2d}/{epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\nError during training: {e}\")\n",
    "        print(\"Training stopped early.\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'test_accs': test_accs,\n",
    "        'final_test_acc': test_accs[-1] if test_accs else 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1_header",
   "metadata": {},
   "source": [
    "## Task 1: The Activation Function Challenge\n",
    "\n",
    "Compare the training loss and accuracy curves when using:\n",
    "- **Sigmoid**: Observe if the model suffers from \"vanishing gradients\" or slow start\n",
    "- **Tanh**: Compare its speed to Sigmoid\n",
    "- **ReLU**: Document why this usually leads to faster convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_sigmoid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Sigmoid Activation\n",
    "model_sigmoid = NeuralNetwork(activation='sigmoid').to(device)\n",
    "optimizer_sigmoid = optim.SGD(model_sigmoid.parameters(), lr=0.01)\n",
    "results_sigmoid = run_experiment(model_sigmoid, optimizer_sigmoid, epochs=10, \n",
    "                                  exp_name=\"Task 1.1: Sigmoid + SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_tanh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Tanh Activation\n",
    "model_tanh = NeuralNetwork(activation='tanh').to(device)\n",
    "optimizer_tanh = optim.SGD(model_tanh.parameters(), lr=0.01)\n",
    "results_tanh = run_experiment(model_tanh, optimizer_tanh, epochs=10, \n",
    "                               exp_name=\"Task 1.2: Tanh + SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_relu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: ReLU Activation\n",
    "model_relu = NeuralNetwork(activation='relu').to(device)\n",
    "optimizer_relu = optim.SGD(model_relu.parameters(), lr=0.01)\n",
    "results_relu = run_experiment(model_relu, optimizer_relu, epochs=10, \n",
    "                               exp_name=\"Task 1.3: ReLU + SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1_analysis",
   "metadata": {},
   "source": [
    "### Task 1: Analysis of Activation Functions\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Sigmoid:**\n",
    "   - Often suffers from vanishing gradient problem\n",
    "   - Outputs are between 0 and 1, which can slow down learning\n",
    "   - May show slower convergence especially in deeper networks\n",
    "\n",
    "2. **Tanh:**\n",
    "   - Outputs are between -1 and 1 (zero-centered)\n",
    "   - Generally faster than Sigmoid due to zero-centered outputs\n",
    "   - Still can suffer from vanishing gradients\n",
    "\n",
    "3. **ReLU:**\n",
    "   - No vanishing gradient problem for positive values\n",
    "   - Faster convergence due to simpler gradient computation\n",
    "   - Helps with sparse activation (some neurons output 0)\n",
    "   - Most commonly used in modern deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Task 1 Results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training Loss Comparison\n",
    "axes[0].plot(results_sigmoid['train_losses'], label='Sigmoid', marker='o')\n",
    "axes[0].plot(results_tanh['train_losses'], label='Tanh', marker='s')\n",
    "axes[0].plot(results_relu['train_losses'], label='ReLU', marker='^')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Task 1: Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Test Accuracy Comparison\n",
    "axes[1].plot(results_sigmoid['test_accs'], label='Sigmoid', marker='o')\n",
    "axes[1].plot(results_tanh['test_accs'], label='Tanh', marker='s')\n",
    "axes[1].plot(results_relu['test_accs'], label='ReLU', marker='^')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test Accuracy (%)')\n",
    "axes[1].set_title('Task 1: Test Accuracy Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Test Accuracies:\")\n",
    "print(f\"Sigmoid: {results_sigmoid['final_test_acc']:.2f}%\")\n",
    "print(f\"Tanh: {results_tanh['final_test_acc']:.2f}%\")\n",
    "print(f\"ReLU: {results_relu['final_test_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2_header",
   "metadata": {},
   "source": [
    "## Task 2: The Optimizer Showdown\n",
    "\n",
    "Using the best activation function (ReLU), compare different optimizers:\n",
    "- **SGD**: Observe the stability of the loss\n",
    "- **SGD with Momentum**: Note how it handles \"bumps\" in the loss landscape\n",
    "- **Adam**: Observe how quickly it reaches high accuracy compared to basic SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_sgd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: ReLU + SGD (without momentum)\n",
    "model_sgd = NeuralNetwork(activation='relu').to(device)\n",
    "optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=0.01)\n",
    "results_sgd = run_experiment(model_sgd, optimizer_sgd, epochs=10, \n",
    "                              exp_name=\"Task 2.1: ReLU + SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: ReLU + SGD with Momentum\n",
    "model_momentum = NeuralNetwork(activation='relu').to(device)\n",
    "optimizer_momentum = optim.SGD(model_momentum.parameters(), lr=0.01, momentum=0.9)\n",
    "results_momentum = run_experiment(model_momentum, optimizer_momentum, epochs=10, \n",
    "                                   exp_name=\"Task 2.2: ReLU + SGD with Momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_adam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6: ReLU + Adam\n",
    "model_adam = NeuralNetwork(activation='relu').to(device)\n",
    "optimizer_adam = optim.Adam(model_adam.parameters(), lr=0.001)\n",
    "results_adam = run_experiment(model_adam, optimizer_adam, epochs=10, \n",
    "                               exp_name=\"Task 2.3: ReLU + Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2_analysis",
   "metadata": {},
   "source": [
    "### Task 2: Analysis of Optimizers\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **SGD (Stochastic Gradient Descent):**\n",
    "   - Simple and straightforward optimization\n",
    "   - May show more oscillations in loss\n",
    "   - Slower convergence compared to adaptive methods\n",
    "\n",
    "2. **SGD with Momentum:**\n",
    "   - Accelerates convergence by accumulating gradients\n",
    "   - Smooths out oscillations in the loss landscape\n",
    "   - Better at escaping local minima\n",
    "\n",
    "3. **Adam (Adaptive Moment Estimation):**\n",
    "   - Combines advantages of momentum and adaptive learning rates\n",
    "   - Usually fastest convergence\n",
    "   - Works well with default hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Task 2 Results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training Loss Comparison\n",
    "axes[0].plot(results_sgd['train_losses'], label='SGD', marker='o')\n",
    "axes[0].plot(results_momentum['train_losses'], label='SGD + Momentum', marker='s')\n",
    "axes[0].plot(results_adam['train_losses'], label='Adam', marker='^')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Task 2: Training Loss Comparison (ReLU)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Test Accuracy Comparison\n",
    "axes[1].plot(results_sgd['test_accs'], label='SGD', marker='o')\n",
    "axes[1].plot(results_momentum['test_accs'], label='SGD + Momentum', marker='s')\n",
    "axes[1].plot(results_adam['test_accs'], label='Adam', marker='^')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test Accuracy (%)')\n",
    "axes[1].set_title('Task 2: Test Accuracy Comparison (ReLU)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Test Accuracies:\")\n",
    "print(f\"SGD: {results_sgd['final_test_acc']:.2f}%\")\n",
    "print(f\"SGD + Momentum: {results_momentum['final_test_acc']:.2f}%\")\n",
    "print(f\"Adam: {results_adam['final_test_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3_header",
   "metadata": {},
   "source": [
    "## Task 3: Batch Normalization and Dropout\n",
    "\n",
    "Run specific scenarios to observe the contrast:\n",
    "- **WITHOUT Batch Normalization and Dropout**\n",
    "- **Without BN, Dropout=0.1**\n",
    "- **With BN, Dropout=0.25**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_scenario1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7: Without BN, Without Dropout\n",
    "model_no_bn_no_drop = NeuralNetwork(activation='relu', use_bn=False, dropout_rate=0.0).to(device)\n",
    "optimizer_7 = optim.Adam(model_no_bn_no_drop.parameters(), lr=0.001)\n",
    "results_no_bn_no_drop = run_experiment(model_no_bn_no_drop, optimizer_7, epochs=10, \n",
    "                                        exp_name=\"Task 3.1: No BN, No Dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_scenario2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 8: Without BN, Dropout=0.1\n",
    "model_no_bn_drop01 = NeuralNetwork(activation='relu', use_bn=False, dropout_rate=0.1).to(device)\n",
    "optimizer_8 = optim.Adam(model_no_bn_drop01.parameters(), lr=0.001)\n",
    "results_no_bn_drop01 = run_experiment(model_no_bn_drop01, optimizer_8, epochs=10, \n",
    "                                       exp_name=\"Task 3.2: No BN, Dropout=0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_scenario3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 9: With BN, Dropout=0.25\n",
    "model_bn_drop025 = NeuralNetwork(activation='relu', use_bn=True, dropout_rate=0.25).to(device)\n",
    "optimizer_9 = optim.Adam(model_bn_drop025.parameters(), lr=0.001)\n",
    "results_bn_drop025 = run_experiment(model_bn_drop025, optimizer_9, epochs=10, \n",
    "                                     exp_name=\"Task 3.3: With BN, Dropout=0.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3_analysis",
   "metadata": {},
   "source": [
    "### Task 3: Analysis of BN and Dropout\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Without BN and Dropout:**\n",
    "   - Baseline performance\n",
    "   - May overfit on training data\n",
    "   - No regularization\n",
    "\n",
    "2. **Without BN, Dropout=0.1:**\n",
    "   - Light regularization\n",
    "   - Helps prevent overfitting\n",
    "   - May slightly reduce training speed\n",
    "\n",
    "3. **With BN, Dropout=0.25:**\n",
    "   - Batch normalization helps with training stability\n",
    "   - Higher dropout provides stronger regularization\n",
    "   - Usually better generalization to test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Task 3 Results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training and Test Accuracy Comparison\n",
    "epochs_range = range(1, 11)\n",
    "\n",
    "axes[0].plot(epochs_range, results_no_bn_no_drop['train_accs'], label='Train - No BN, No Dropout', linestyle='-', marker='o')\n",
    "axes[0].plot(epochs_range, results_no_bn_no_drop['test_accs'], label='Test - No BN, No Dropout', linestyle='--', marker='o')\n",
    "axes[0].plot(epochs_range, results_no_bn_drop01['train_accs'], label='Train - No BN, Dropout=0.1', linestyle='-', marker='s')\n",
    "axes[0].plot(epochs_range, results_no_bn_drop01['test_accs'], label='Test - No BN, Dropout=0.1', linestyle='--', marker='s')\n",
    "axes[0].plot(epochs_range, results_bn_drop025['train_accs'], label='Train - BN, Dropout=0.25', linestyle='-', marker='^')\n",
    "axes[0].plot(epochs_range, results_bn_drop025['test_accs'], label='Test - BN, Dropout=0.25', linestyle='--', marker='^')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('Task 3: Training vs Test Accuracy')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Training Loss Comparison\n",
    "axes[1].plot(results_no_bn_no_drop['train_losses'], label='No BN, No Dropout', marker='o')\n",
    "axes[1].plot(results_no_bn_drop01['train_losses'], label='No BN, Dropout=0.1', marker='s')\n",
    "axes[1].plot(results_bn_drop025['train_losses'], label='BN, Dropout=0.25', marker='^')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Training Loss')\n",
    "axes[1].set_title('Task 3: Training Loss Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Test Accuracies:\")\n",
    "print(f\"No BN, No Dropout: {results_no_bn_no_drop['final_test_acc']:.2f}%\")\n",
    "print(f\"No BN, Dropout=0.1: {results_no_bn_drop01['final_test_acc']:.2f}%\")\n",
    "print(f\"With BN, Dropout=0.25: {results_bn_drop025['final_test_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_header",
   "metadata": {},
   "source": [
    "## Comprehensive Comparison Table\n",
    "\n",
    "Summary of all experiments conducted in this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Experiment': 1,\n",
    "        'Activation': 'Sigmoid',\n",
    "        'Optimizer': 'SGD',\n",
    "        'BN': 'No',\n",
    "        'Dropout': 0.0,\n",
    "        'Epochs': 10,\n",
    "        'Final Test Accuracy (%)': f\"{results_sigmoid['final_test_acc']:.2f}\"\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 2,\n",
    "        'Activation': 'Tanh',\n",
    "        'Optimizer': 'SGD',\n",
    "        'BN': 'No',\n",
    "        'Dropout': 0.0,\n",
    "        'Epochs': 10,\n",
    "        'Final Test Accuracy (%)': f\"{results_tanh['final_test_acc']:.2f}\"\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 3,\n",
    "        'Activation': 'ReLU',\n",
    "        'Optimizer': 'SGD',\n",
    "        'BN': 'No',\n",
    "        'Dropout': 0.0,\n",
    "        'Epochs': 10,\n",
    "        'Final Test Accuracy (%)': f\"{results_relu['final_test_acc']:.2f}\"\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 4,\n",
    "        'Activation': 'ReLU',\n",
    "        'Optimizer': 'SGD + Momentum',\n",
    "        'BN': 'No',\n",
    "        'Dropout': 0.0,\n",
    "        'Epochs': 10,\n",
    "        'Final Test Accuracy (%)': f\"{results_momentum['final_test_acc']:.2f}\"\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 5,\n",
    "        'Activation': 'ReLU',\n",
    "        'Optimizer': 'Adam',\n",
    "        'BN': 'No',\n",
    "        'Dropout': 0.0,\n",
    "        'Epochs': 10,\n",
    "        'Final Test Accuracy (%)': f\"{results_adam['final_test_acc']:.2f}\"\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 6,\n",
    "        'Activation': 'ReLU',\n",
    "        'Optimizer': 'Adam',\n",
    "        'BN': 'No',\n",
    "        'Dropout': 0.1,\n",
    "        'Epochs': 10,\n",
    "        'Final Test Accuracy (%)': f\"{results_no_bn_drop01['final_test_acc']:.2f}\"\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 7,\n",
    "        'Activation': 'ReLU',\n",
    "        'Optimizer': 'Adam',\n",
    "        'BN': 'Yes',\n",
    "        'Dropout': 0.25,\n",
    "        'Epochs': 10,\n",
    "        'Final Test Accuracy (%)': f\"{results_bn_drop025['final_test_acc']:.2f}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: Run all cells above in sequence to generate experiment results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_viz_header",
   "metadata": {},
   "source": [
    "## Final Visualizations\n",
    "\n",
    "Comprehensive plots showing loss curves and accuracy for different experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization of all experiments\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Task 1 - Activation Functions (Loss)\n",
    "axes[0, 0].plot(results_sigmoid['train_losses'], label='Sigmoid - Train', linestyle='-', marker='o')\n",
    "axes[0, 0].plot(results_sigmoid['test_losses'], label='Sigmoid - Test', linestyle='--', marker='o')\n",
    "axes[0, 0].plot(results_tanh['train_losses'], label='Tanh - Train', linestyle='-', marker='s')\n",
    "axes[0, 0].plot(results_tanh['test_losses'], label='Tanh - Test', linestyle='--', marker='s')\n",
    "axes[0, 0].plot(results_relu['train_losses'], label='ReLU - Train', linestyle='-', marker='^')\n",
    "axes[0, 0].plot(results_relu['test_losses'], label='ReLU - Test', linestyle='--', marker='^')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Task 1: Loss Curves - Activation Functions')\n",
    "axes[0, 0].legend(fontsize=8)\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot 2: Task 1 - Activation Functions (Accuracy)\n",
    "axes[0, 1].plot(results_sigmoid['train_accs'], label='Sigmoid - Train', linestyle='-', marker='o')\n",
    "axes[0, 1].plot(results_sigmoid['test_accs'], label='Sigmoid - Test', linestyle='--', marker='o')\n",
    "axes[0, 1].plot(results_tanh['train_accs'], label='Tanh - Train', linestyle='-', marker='s')\n",
    "axes[0, 1].plot(results_tanh['test_accs'], label='Tanh - Test', linestyle='--', marker='s')\n",
    "axes[0, 1].plot(results_relu['train_accs'], label='ReLU - Train', linestyle='-', marker='^')\n",
    "axes[0, 1].plot(results_relu['test_accs'], label='ReLU - Test', linestyle='--', marker='^')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Task 1: Accuracy - Activation Functions')\n",
    "axes[0, 1].legend(fontsize=8)\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot 3: Task 2 - Optimizers (Loss)\n",
    "axes[1, 0].plot(results_sgd['train_losses'], label='SGD - Train', linestyle='-', marker='o')\n",
    "axes[1, 0].plot(results_sgd['test_losses'], label='SGD - Test', linestyle='--', marker='o')\n",
    "axes[1, 0].plot(results_momentum['train_losses'], label='SGD+Momentum - Train', linestyle='-', marker='s')\n",
    "axes[1, 0].plot(results_momentum['test_losses'], label='SGD+Momentum - Test', linestyle='--', marker='s')\n",
    "axes[1, 0].plot(results_adam['train_losses'], label='Adam - Train', linestyle='-', marker='^')\n",
    "axes[1, 0].plot(results_adam['test_losses'], label='Adam - Test', linestyle='--', marker='^')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Task 2: Loss Curves - Optimizers (ReLU)')\n",
    "axes[1, 0].legend(fontsize=8)\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot 4: Task 2 - Optimizers (Accuracy)\n",
    "axes[1, 1].plot(results_sgd['train_accs'], label='SGD - Train', linestyle='-', marker='o')\n",
    "axes[1, 1].plot(results_sgd['test_accs'], label='SGD - Test', linestyle='--', marker='o')\n",
    "axes[1, 1].plot(results_momentum['train_accs'], label='SGD+Momentum - Train', linestyle='-', marker='s')\n",
    "axes[1, 1].plot(results_momentum['test_accs'], label='SGD+Momentum - Test', linestyle='--', marker='s')\n",
    "axes[1, 1].plot(results_adam['train_accs'], label='Adam - Train', linestyle='-', marker='^')\n",
    "axes[1, 1].plot(results_adam['test_accs'], label='Adam - Test', linestyle='--', marker='^')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "axes[1, 1].set_title('Task 2: Accuracy - Optimizers (ReLU)')\n",
    "axes[1, 1].legend(fontsize=8)\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_final_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 Comprehensive Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "axes[0].plot(results_no_bn_no_drop['train_losses'], label='No BN, No Dropout - Train', linestyle='-', marker='o')\n",
    "axes[0].plot(results_no_bn_no_drop['test_losses'], label='No BN, No Dropout - Test', linestyle='--', marker='o')\n",
    "axes[0].plot(results_no_bn_drop01['train_losses'], label='No BN, Dropout=0.1 - Train', linestyle='-', marker='s')\n",
    "axes[0].plot(results_no_bn_drop01['test_losses'], label='No BN, Dropout=0.1 - Test', linestyle='--', marker='s')\n",
    "axes[0].plot(results_bn_drop025['train_losses'], label='BN, Dropout=0.25 - Train', linestyle='-', marker='^')\n",
    "axes[0].plot(results_bn_drop025['test_losses'], label='BN, Dropout=0.25 - Test', linestyle='--', marker='^')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Task 3: Loss Curves - BN and Dropout Effects')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot 2: Accuracy curves\n",
    "axes[1].plot(results_no_bn_no_drop['train_accs'], label='No BN, No Dropout - Train', linestyle='-', marker='o')\n",
    "axes[1].plot(results_no_bn_no_drop['test_accs'], label='No BN, No Dropout - Test', linestyle='--', marker='o')\n",
    "axes[1].plot(results_no_bn_drop01['train_accs'], label='No BN, Dropout=0.1 - Train', linestyle='-', marker='s')\n",
    "axes[1].plot(results_no_bn_drop01['test_accs'], label='No BN, Dropout=0.1 - Test', linestyle='--', marker='s')\n",
    "axes[1].plot(results_bn_drop025['train_accs'], label='BN, Dropout=0.25 - Train', linestyle='-', marker='^')\n",
    "axes[1].plot(results_bn_drop025['test_accs'], label='BN, Dropout=0.25 - Test', linestyle='--', marker='^')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Task 3: Accuracy - BN and Dropout Effects')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary of Findings:\n",
    "\n",
    "**Task 1 - Activation Functions:**\n",
    "- ReLU generally performs best due to avoiding vanishing gradients\n",
    "- Sigmoid and Tanh can suffer from slower convergence\n",
    "- ReLU is the most commonly used activation in modern deep learning\n",
    "\n",
    "**Task 2 - Optimizers:**\n",
    "- Adam typically converges fastest\n",
    "- SGD with Momentum provides better stability than basic SGD\n",
    "- The choice of optimizer significantly impacts training speed\n",
    "\n",
    "**Task 3 - Regularization:**\n",
    "- Batch Normalization helps with training stability\n",
    "- Dropout prevents overfitting\n",
    "- Combining BN and Dropout usually gives best generalization\n",
    "\n",
    "**Best Configuration:**\n",
    "- Activation: ReLU\n",
    "- Optimizer: Adam\n",
    "- Regularization: Batch Normalization with moderate Dropout\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}