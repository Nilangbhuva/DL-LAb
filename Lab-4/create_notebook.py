import json

# Create a comprehensive Lab-4 notebook structure
notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 4: CNN Architectures for Imbalanced Image Classification\n",
                "\n",
                "**Student Information:**\n",
                "- **Name:** Nilang Bhuva\n",
                "- **Admission Number:** U23AI047\n",
                "- **Year:** 3rd Year\n",
                "- **Program:** Artificial Intelligence (AI)\n",
                "\n",
                "## Overview\n",
                "\n",
                "This lab implements CNN architectures for handling imbalanced image classification across multiple benchmark datasets. We address seven comprehensive problem statements covering architecture design, imbalance handling, comparative analysis, loss functions, feature visualization, transfer learning, and error analysis."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Table of Contents\n",
                "\n",
                "1. [Setup and Imports](#setup)\n",
                "2. [Problem Statement 1: Architecture Design Focus](#ps1)\n",
                "3. [Problem Statement 2: Imbalanced Dataset Handling](#ps2)\n",
                "4. [Problem Statement 3: Comparative Architecture Analysis](#ps3)\n",
                "5. [Problem Statement 4: Loss Function & Optimization Challenge](#ps4)\n",
                "6. [Problem Statement 5: Feature Representation & Visualization](#ps5)\n",
                "7. [Problem Statement 6: Generalization & Transfer Learning](#ps6)\n",
                "8. [Problem Statement 7: Error Analysis & Improvement](#ps7)\n",
                "9. [Summary and Conclusions](#summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='setup'></a>\n",
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard library imports\n",
                "import os\n",
                "import random\n",
                "import time\n",
                "from collections import Counter, defaultdict\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Data manipulation and numerical computing\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# PyTorch imports\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Subset\n",
                "from torchvision import datasets, transforms, models\n",
                "\n",
                "# Scikit-learn\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    confusion_matrix, classification_report, roc_auc_score,\n",
                "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
                "    balanced_accuracy_score\n",
                ")\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import label_binarize\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Imbalanced learning\n",
                "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
                "from imblearn.under_sampling import RandomUnderSampler\n",
                "\n",
                "# UMAP for dimensionality reduction\n",
                "try:\n",
                "    import umap\n",
                "    UMAP_AVAILABLE = True\n",
                "except ImportError:\n",
                "    UMAP_AVAILABLE = False\n",
                "    print(\"UMAP not available. Install with: pip install umap-learn\")\n",
                "\n",
                "# Progress bar\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "def set_seed(seed=42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "\n",
                "set_seed(42)\n",
                "\n",
                "# Device configuration\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "# Set plot style\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='ps1'></a>\n",
                "## 2. Problem Statement 1: Architecture Design Focus\n",
                "\n",
                "**Objective:** Design custom CNN architecture for imbalanced image classification\n",
                "\n",
                "### 2.1 Dataset Preparation - Creating Imbalanced Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to create imbalanced CIFAR-10 dataset\n",
                "def create_imbalanced_cifar10(imbalance_ratio=100, data_dir='./data'):\n",
                "    \"\"\"\n",
                "    Create imbalanced CIFAR-10 dataset with long-tailed distribution.\n",
                "    \n",
                "    Args:\n",
                "        imbalance_ratio: Ratio between majority and minority class\n",
                "        data_dir: Directory to save/load dataset\n",
                "    \n",
                "    Returns:\n",
                "        train_dataset: Imbalanced training dataset\n",
                "        test_dataset: Original balanced test dataset\n",
                "        class_counts: Distribution of samples per class\n",
                "    \"\"\"\n",
                "    # Define transforms\n",
                "    transform_train = transforms.Compose([\n",
                "        transforms.RandomCrop(32, padding=4),\n",
                "        transforms.RandomHorizontalFlip(),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
                "    ])\n",
                "    \n",
                "    transform_test = transforms.Compose([\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
                "    ])\n",
                "    \n",
                "    # Load full CIFAR-10 dataset\n",
                "    full_train = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n",
                "    test_dataset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n",
                "    \n",
                "    # Create imbalanced distribution\n",
                "    targets = np.array(full_train.targets)\n",
                "    classes = np.unique(targets)\n",
                "    num_classes = len(classes)\n",
                "    \n",
                "    # Calculate samples per class for long-tailed distribution\n",
                "    max_samples = 5000  # Maximum samples for majority class\n",
                "    samples_per_class = []\n",
                "    for i in range(num_classes):\n",
                "        # Exponential decay for long-tailed distribution\n",
                "        n_samples = int(max_samples * (imbalance_ratio ** (-i / (num_classes - 1))))\n",
                "        samples_per_class.append(n_samples)\n",
                "    \n",
                "    # Select indices for imbalanced dataset\n",
                "    selected_indices = []\n",
                "    class_counts = {}\n",
                "    \n",
                "    for cls, n_samples in enumerate(samples_per_class):\n",
                "        cls_indices = np.where(targets == cls)[0]\n",
                "        selected = np.random.choice(cls_indices, size=min(n_samples, len(cls_indices)), replace=False)\n",
                "        selected_indices.extend(selected)\n",
                "        class_counts[cls] = len(selected)\n",
                "    \n",
                "    # Create imbalanced dataset\n",
                "    train_dataset = Subset(full_train, selected_indices)\n",
                "    \n",
                "    return train_dataset, test_dataset, class_counts\n",
                "\n",
                "# Load and create imbalanced CIFAR-10\n",
                "print(\"Creating Imbalanced CIFAR-10 Dataset...\")\n",
                "cifar_train, cifar_test, cifar_class_counts = create_imbalanced_cifar10(imbalance_ratio=100)\n",
                "\n",
                "print(\"\\nCIFAR-10 Class Distribution:\")\n",
                "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
                "for cls, count in cifar_class_counts.items():\n",
                "    print(f\"  {class_names[cls]}: {count} samples\")\n",
                "print(f\"\\nTotal training samples: {len(cifar_train)}\")\n",
                "print(f\"Total test samples: {len(cifar_test)}\")\n",
                "print(f\"Imbalance ratio: {max(cifar_class_counts.values()) / min(cifar_class_counts.values()):.2f}:1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize class distribution\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "classes = list(cifar_class_counts.keys())\n",
                "counts = list(cifar_class_counts.values())\n",
                "plt.bar([class_names[i] for i in classes], counts, color='steelblue')\n",
                "plt.xlabel('Class')\n",
                "plt.ylabel('Number of Samples')\n",
                "plt.title('CIFAR-10 Imbalanced Distribution (100:1 ratio)')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.bar([class_names[i] for i in classes], counts, color='coral', log=True)\n",
                "plt.xlabel('Class')\n",
                "plt.ylabel('Number of Samples (log scale)')\n",
                "plt.title('CIFAR-10 Distribution (Log Scale)')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('cifar10_class_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Custom CNN Architecture Design\n",
                "\n",
                "Design a custom CNN with:\n",
                "- Multiple convolutional blocks with increasing channels\n",
                "- Batch normalization for training stability\n",
                "- Dropout for regularization\n",
                "- Appropriate activation functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CustomCNN(nn.Module):\n",
                "    \"\"\"\n",
                "    Custom CNN architecture designed for imbalanced image classification.\n",
                "    \n",
                "    Architecture:\n",
                "    - Conv Block 1: 3 -> 64 channels\n",
                "    - Conv Block 2: 64 -> 128 channels\n",
                "    - Conv Block 3: 128 -> 256 channels\n",
                "    - Conv Block 4: 256 -> 512 channels\n",
                "    - Global Average Pooling\n",
                "    - Fully Connected Layers with Dropout\n",
                "    \n",
                "    Regularization:\n",
                "    - Batch Normalization after each conv layer\n",
                "    - Dropout (p=0.5) in FC layers\n",
                "    - L2 weight decay (applied through optimizer)\n",
                "    \"\"\"\n",
                "    def __init__(self, num_classes=10, dropout=0.5):\n",
                "        super(CustomCNN, self).__init__()\n",
                "        \n",
                "        # Convolutional Block 1\n",
                "        self.conv1 = nn.Sequential(\n",
                "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(64),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(64),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        )\n",
                "        \n",
                "        # Convolutional Block 2\n",
                "        self.conv2 = nn.Sequential(\n",
                "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(128),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(128),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        )\n",
                "        \n",
                "        # Convolutional Block 3\n",
                "        self.conv3 = nn.Sequential(\n",
                "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(256),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(256),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        )\n",
                "        \n",
                "        # Convolutional Block 4\n",
                "        self.conv4 = nn.Sequential(\n",
                "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(512),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(512),\n",
                "            nn.ReLU(inplace=True)\n",
                "        )\n",
                "        \n",
                "        # Global Average Pooling\n",
                "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
                "        \n",
                "        # Fully Connected Layers\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(512, 256),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(256, num_classes)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.conv1(x)\n",
                "        x = self.conv2(x)\n",
                "        x = self.conv3(x)\n",
                "        x = self.conv4(x)\n",
                "        x = self.gap(x)\n",
                "        x = x.view(x.size(0), -1)\n",
                "        x = self.fc(x)\n",
                "        return x\n",
                "    \n",
                "    def extract_features(self, x):\n",
                "        \"\"\"Extract features before final FC layer for visualization\"\"\"\n",
                "        x = self.conv1(x)\n",
                "        x = self.conv2(x)\n",
                "        x = self.conv3(x)\n",
                "        x = self.conv4(x)\n",
                "        x = self.gap(x)\n",
                "        x = x.view(x.size(0), -1)\n",
                "        return x\n",
                "\n",
                "# Test the model\n",
                "model = CustomCNN(num_classes=10).to(device)\n",
                "print(\"Custom CNN Architecture:\")\n",
                "print(model)\n",
                "\n",
                "# Count parameters\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"\\nTotal parameters: {total_params:,}\")\n",
                "print(f\"Trainable parameters: {trainable_params:,}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

with open('Lab-4.ipynb', 'w') as f:
    json.dump(notebook, f, indent=2)

print("Notebook created successfully!")
